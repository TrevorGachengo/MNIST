#  2-Layer Neural Network From Scratch  | MNIST Database 

## Contents
_to be added_
### Activation functions
- Hidden layer: ReLU
- Output layer: Sigmoid
### Optimizer
- Adam

### Stochastic gradient descent (SGD)

### interfacer


## Motivation for project
_to be added_
- yt vid and learning

  
## References
_to be added_

- **The Complete Mathematics of Neural Networks and Deep Learning.** https://www.youtube.com/watch?v=Ixl3nykKG9M 

- http://neuralnetworksanddeeplearning.com/chap1.html
- https://github.com/kdexd/digit-classifier/blob/master/network.py
