#  2-Layer Neural Network From Scratch  | MNIST Database 

## Contents
### Interfacer
- This module opens a canvas that allows users to draw digits, which the network then attempts to classify based on the training it received on the MNIST dataset.

### Activation functions
- Hidden layer: ReLU
- Output layer: Sigmoid

### Optimizer
- Adam


## Motivation for project
I initially aimed to create a network capable of solving a Rubik's cube, but I realized I had no foundational knowledge of how neural networks actually worked. This led me to the idea of building a multiclass classification model from scratch, without using libraries like PyTorch or TensorFlow. This approach has allowed me to grasp the core fundamentals of neural networks and further develop my problem-solving skills.

  
## References
- **The Complete Mathematics of Neural Networks and Deep Learning.** https://www.youtube.com/watch?v=Ixl3nykKG9M 

- http://neuralnetworksanddeeplearning.com/chap1.html
- https://github.com/kdexd/digit-classifier/blob/master/network.py
